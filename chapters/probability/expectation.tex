\chapter{Expectations and Invariance}
\section{Expectation of a Random Variable}
The distribution of $X$ contains all the probabilistic data we need about $X$. However, we need additional tools to describe these data more cleanly. 
\par
One of these tools is \textbf{Expectation}, or \textbf{Expected Value} or \textbf{Mean} of $X$.
\begin{definition}
    The \textbf{expected value} of $X$ is defined as,
    \[\op{E}(X)= \begin{cases}
        \sum xf(x)& \text{if $X$ is discrete} \\
        \int xf(x)dx &\text{if $X$ is continous}
    \end{cases}\]
    If expected value is infinite, we say that expected value of $X$ doesn't exist.\\
    We can also combinte both the notations into a whole generalized equation with a notation,
    \[\op{E}(X) = \int x dF(x)= \mu = \mu_X\]
    We have discussed that $dF(x) = f(X)$. in the second chapter.
    \textbf{Important Note}. Expectation, by nature, is a theorical mean of the variables we get. It is sometimes possible to get mean that you can't get in a practical settings.
\end{definition}
By definition of probability, sum of all $f(x)$ is is simply $1$. Then, the above equation is weighted mean of $X$, which is what we wanted to convey.

\begin{example}
    Suppose that we have  a discrete r.v $X$ to describe the probability of getting heads from tossing a coin $3$ times.  Let  c.d.f of $X$ be $f$. Then,
    \[X = \begin{cases}
        &f(0) = 1/8\\
        &f(1) = 3/8 \\
        &f(2) = 3/8 \\
        &f(3) = 1/8
    \end{cases}\]
    Let's use our above formula to calculate $\E(X)$,
    \[\E(X) =  \frac{1}{8} \cdot 0 + \frac{3}{8} \cdot 1 + \frac{3}{8} \cdot 2 + \frac{1}{8} \cdot 3 = 1.5\]
    This number shows that if we repeat our experiment for a very long time, the mean of the heads we got woud be  ( or approach to) $1.5$.
    \\Observe that weighted mean is equivalent to arithmetic mean. Because gettings $X=2$ is simply getting $X=1$ two times.
\end{example}
\par
\vspace{10cm}
But, what if $Y= g(X)$ and we want to compute $E(Y)$? We have a theorem for that,
\begin{theorem}[Law of the Unconscious Statistician]
    Let $Y = g(X)$. Then,
    \[\op{E}(Y)= \E (g(X))= \int g(x) dF_X(x)\]
       The general proof of this theorem is out of the scope of this book. Comparing this to original expectation equation, we can see that the only thing that changes is $g(x)$ and $x$, which intuitively makes sense if you think about it. In transformations, probabiltiies remains unchanged, while the result of probabiltiies gets transformed by a function.
       \\ Moreover, for a special case $g(x) = I_A(x)$, where $I_A(x) \in \{ 0,1\}$ depending on $x \in A$, then,
       \[\E (I_A(X)) = \int I_A(x) d F_X(x) = \int_A dF_X(x) = \op{P} (X \in A)\]
       This means that probability is special case of expectation, which makes sense, considering probability itself is some average by definition.
\end{theorem}
\begin{definition}
We call  $n$-th \textbf{ raw moment} of $X$ as 
\[ \mu_n = \E (X^n) = \int x^n dF_X(x)\] 
If $ \E ( |X^k|)$ is infinite, then $k^{th}$ moment do not exist.\\
We also define \textbf{ $k$-th central moment} as moments about its mean $\mu$ i.e $\E [(X - \mu)^k   ]$. Additionally, \textbf{ $k$-th standardized moments} as $\frac{\E [ (X - \mu)^n]}{\sigma^n}$.
\par
The $1$st moment, the $2$nd central moment, $3$rd  and $4$rd standarized moments are called mean (expected value), \textbf{variance} ,\textbf{skewness} and \textbf{kurtosis} in order. We will learn more about them in later chapters.

\par 
The moments are very useful and practical. Although there  are infinitely many moments, only smaller ones are important for practical purposes. We already know the first moment and its significance. 
\end{definition}
\subsection*{Properties of Expectation}
\begin{theorem}[Non-negativity] If $X \ge 0$ is a r.v, then $\E (X) \ge 0$.
    \begin{proof}
        By definition of expectation, we have
        \[ \E(X) = \int x dF_X(x)  \ge 0 \]
        since by definition, $dF_X(X) \ge 0$ and $x \ge 0$.
    \end{proof}
\end{theorem}
\begin{theorem}[Linearity] For \textbf{any} random variables $X_1,X_2,...,X_n$ and constants $a_1,a_2,...,a_n$, we have 
    \[\E \bigg[ \sum^n_i a_iX_i \bigg] = \sum^n_i a_i\E({X_i})\]
    \begin{proof}
        We will first prove the theorem for $n=2$ with $X,Y$.. $n=1$ is trivial.
        \begin{align*}
            \E \bigg[ a_1X_1 + a_2Y\bigg] &= \int (a_1x+a_2y) dF_{X,Y}(x,y) \\
                                            &=\int (a_1x)dF_{X}(x) + \int (a_2y)dF_{Y}(y)\\
                                            &= a_1\int x dF_{X}(x) + a_2 \int y dF_{X}(y) \\
                                            &= a_1\E(X) + a_2\E(Y)
        \end{align*}
        The second line is the direct consequence of marginality.
        With induction, $n\ge 3$ is also true, however I will omit the solution for the sake of briefity.
    \end{proof}
    This theorem is very useful and very practical.
\end{theorem}
\begin{theorem}[multiplicity]
    For \textbf{independent} r.v $X_1,X_2,...,X_n$,  we have
    \[\E \bigg( \prod _{i=1}^nX_i \bigg) = \prod_{i=1}^n \E(X_i)\]
    \begin{proof}
        Similiar to last one, we will use induction. $n=1$ is trivial. For $n=2$, let r.v be $X,Y$. Remember that independence has property $dF_{X,Y}(x,y)= dF_{X}(x)\cdot dF_{Y}(y)$.
        \[\E(XY) = \int (xy)dF_{X,Y}(x,y) = \int xydF_{X}(x)dF_{Y}(y)= \int ydF_Y(y)\int xdF_X(x)= \E(X)\E(Y)\]
        For the sake of briefity, I won't show the induction part.
    \end{proof}
\end{theorem}
\section{Conditional Expectation}
Suppose that we we want to calculate mean of $X$ when $Y=y$. This is called conditional expectation, similar to conditional r.v and probability.
\begin{definition} \textbf{conditional expectation} of $X$ by $Y=y$ is given by,
\[\E(X | Y=y) = \int x dF_{X|Y}(x|y)\]
Note that $\E(X|Y)$ is a r.v itself since we don't know value of $Y$ beforehand, or more precisely $Y$ is a "function".
\end{definition}
\begin{theorem}[ Law of total Expectations] for all r.v $X$ and $Y$,
    \[\E[\E(Y|X)] = \E(Y) \qquad \text{and} \qquad  \E[\E(X|Y)] = \E(X)\]
    \begin{proof}
        It is direct consequence of definition of conditional expectation and the fact that $dF(x,y) = dF(x)dF(y|x)$
        \[ \E[\E(Y|X)] = \int \E(Y|X=x) dF(x)\]writelater
    \end{proof}
\end{theorem}
\section{Variance}
We have dicussed about the expectation, a way of showing a property of a distribution. However, the expectation alone doesn't convey much. We have another tool called `\textbf{Variance}'.
Variance, in layman terms, describes how value of random variable varies are spread in the graph. Or in other terms, the distance between the expectation value. 
\par

We can define variance ass,
\begin{definition}
    Let $X$ be a r.v with mean $\mu = \E(X)$. The Variance of $X$, denoted as $\V(X)$ or  Var$(X)$ or $\sigma ^2$ is the $2^{nd}$ central moment and is defined by,
    \[ \sigma^2 = \E [(X- \mu)^2] = \int (x -\mu)^2 dF(x)\]
    
    We also define \textbf{standart deviation} as sd$(X) = \sqrt{\sigma^2}= \sigma$.
\end{definition}
The standart deviation and variance convey the same information. They both represent the spread of our data. 
The difference between them is purely mathematical. The variance is more useful in mathematical applications, where standart deviation is very intuitive and practical. 
\href{https://www.mathsisfun.com/data/standard-deviation.html}{mathisfun} explains it very well.
\par
Calculating variance directly can be complicated  and tedious directly sometimes. We can derive a theorem from the original definition for practical purposes.
\begin{theorem}
    Let $X$ be a random variable. Then,
    \[\sigma^2 = E([ (X-\mu)^2]) = E(X^2) - \mu^2\]
    \begin{proof}
        It is derived directly by algebraic manipulation and basic calculus,
        \begin{align*}
            \sigma^2 &= \int (x- \mu)^2 dF(x)  \\
                     &= \int x^2 dF(x) -2\mu \int x dF(x) + \mu^2 \int dF(x) \\
                     &= \int x^2 dF(x) - \mu^2 \\
                     &= \E(X^2) - \mu^2
        \end{align*}
    \end{proof}
\end{theorem}
\section{Conditional Variance}
\begin{definition}
    Let $\mu = \E (X |Y=y)$. The \textbf{conditional variance} is defined as,
    \[ \V(X| Y=y) = \int (x- \mu)^2 dF_{X|Y}(x|y)\]
    The conditional variance tells us how much of spread is left after  We use $Y=y$. Reminder that $\V(X|Y)$ is a r.v itself since Y is a sort of "function" here.
\end{definition}    
\begin{theorem}[Law of Total Variance] for any r.v $X,Y$, it is always true that,
    \[\V(Y)= \E\big[\V(Y|X)\big]+ \V\big(\E[Y|X]\big)\]
    We have stated before that $V(Y|X)$ and $E(Y|X)$ are random variables, not numbers. Therefore We compute variance and expectation of these random variables, and add them up to get the variance $V(Y)$.
\end{theorem}
\section{Covariance and Corelation}
Ley $X$ and $Y$ be r.v.  \textbf{ Covariance} and \textbf{ Corelation} describes the linear relationship between $X$ and $Y$.
\begin{definition}
    If $X$ and $Y$ are r.v with mean $\mu_X$, $\mu_Y$ and standart deviations $\sigma_X$, $\sigma_Y$, we define \textbf{covariance} as,
    \[\mathrm{Cov}(X,Y) = \E \bigg((X- \mu_X)(Y - \mu_Y) \bigg)\]
    and \textbf{corelation} as,
    \[\rho = \rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X\sigma_Y}\]
    Notice that $\mathrm{Cov}(X,X) = \op{V}(X)$ and $\rho_{X,X} = 1$.
\end{definition}
Similiar to variance, calculatig covariance can be tedious. We can derive a better formula by simple algebraic manipulations,
\begin{theorem}
    For all random variables with non-infinite means, we have
    \[\mathrm{Cov}{(X,Y) = \E(XY)-\E(X)\E(Y)}\]
    \begin{proof}
        Similar to Variance one, we have,
        \begin{align*}
                    \mathrm{Cov(X,Y)}  &= \E \bigg( (X- \mu_X)(Y - \mu_Y) \bigg)\\
                                        &= \E ( XY- X\mu_Y-Y\mu_X+\mu_X\mu_Y ) \\
                                        &= \E(XY) - \mu_Y\E(X) - \mu_X\E(Y) + \mu_X\mu_Y \\ 
                                        &= \E(XY) - \E(X)\E(Y)
        \end{align*}
    \end{proof}
\end{theorem}
\begin{theorem}
    For all random variables with non-infinite means, we have,
    \[ -1 \le \rho_{X,Y} \le 1\]
    \begin{proof}
        It is direct consequence of Cauchy-Schwarz inequality.
    \end{proof}
\end{theorem}
