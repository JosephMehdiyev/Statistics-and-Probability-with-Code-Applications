\chapter{Random Variables}
``A random variable can be compared with the holy roman empire: The Holy Roman Empire was not holy, it was not roman, and it was not an
empire.'' \\

%----------------------------------------------------------

\section{Introduction to Random Variables}

A Random Variable describes the data or the outcome $\omega$ as a real number.

\begin{definition}[Random Variables]
    A \alert{Random Variable X} is a function,
    $$ X \ : \ \Omega \rightarrow \mathbb{R}$$
    That assigns a real number $X(\omega)$ to each outcome $\omega$. \\
\end{definition}

\begin{example} Flip a coin. We know that $\Omega =\{H,T\}$. A r.v $X$ might assign $X(H) = 1$ and $X(T) = 0$. That is, heads is ``coded''
    as 1 and tails is ``coded" as 0.

\end{example}
\begin{example}
    Flip a fair coin $n$ times. Let $X$ represent the number of heads we get. Then, $X$ is a random variable that takes values $\{0,1,2,...,n\}$.
\end{example}

\begin{example}
    Toss a fair six sided dice 2 times. Let $X$ be the sum of the two rolls we get. Then, $X$ is a random variable that takes values
    $\{2,3,4,...,12\}$.
\end{example}

\begin{example}
    A students wants to write a real number in intervals $[0,1]$. Let $X$ be the number student writes. Then, $X$ is also a random variable
    that takes any real numbers in that interval.
\end{example}

Random Variables also have \textit{Independence, Conditional Random Variable, a
probability function} and so on. Additionally , Random Variables can be either \alert{Discrete} or \alert{Continous}.
\par
Discrete Random Variable's range is finite or countably infinite. The first two examples we gave are Discrete. Continuous Random
Variables's range is uncountably infinite like the third example \newline

\section{Distribution Functions CDF, PMF, PDF, PPF}
\subsection*{CDF and PPF}
We define  \alert{Cumulative Distribution Function} as,

\begin{definition}{CDF}
    The \alert{Cumulative Distribution Function} or shortly \alert{CDF} is a function $F_X \ : \ \mathbb{R} \rightarrow [0,1]$ such that
    \[ F_X(x) = P( X \le x) \]
\end{definition}

\begin{example}
    We toss a fair coin two times. Let $X$ represent the number of heads we get. Then CDF of $X$ is,
    \[F_X(x) =
        \begin{cases}
            0 \qquad &x < 0\\
            1/4 \qquad &0 \le x <1 \\
            3/4 \qquad &1 \leq x < 2 \\
            1 \qquad &x \ge 2
        \end{cases}
    \]
\end{example}

Now, let's look at some properties of CDF,
\begin{theorem}
    Let $X$ have CDF $F$, and $Y$  have CDF $G$. If $F(x)=G(x)$ for all $x$, then,
    \[ P(X \in A) = P(Y \in A) \quad \text{for all $A$} \]
\end{theorem}

\begin{theorem}
    the function $F \ : \mathbb{R} \rightarrow [0,1]$ is a CDF for some r.v if and only if $F$ satisfies three conditions:
    \begin{enumerate}
        \item $F$ is non-decreasing
        \item $F$ is normalized i.e \[ \lim_{x \rightarrow -\infty} F(x) = 0\ \ \land \ \lim_{x \rightarrow \infty} F(x) =1 \]
        \item $F$ is right continuous.
    \end{enumerate}
\end{theorem}
\begin{definition}
    \textbf{Quantile Percent point function}, or shortly \alert{PPF}, is defined as inverse of CDF i.e,
    \[Q(x) = F^{-1}(x)\]
\end{definition}

\subsection*{PMF and PDF}
Similar to probabilities of Events, we can calculate probability of $X$, depending on discrete or Continuous with functions called
\alert{Probability Mass Function} and \alert{Probability Density Function}, shortly PMF and PDF respectively.
\begin{definition}
    If $X$ is discrete, and it takes \textit{countably} values $ \{ x_1,x_2,..,
    x_n \}$ we define \textbf{Probability Mass Function} of X as follows:
    $$f_X(x)= P(X = x)$$
    Note that $P(X = x)$ is a function, not a number. We have to specify $x$ first to get a number.
\end{definition}

With the properties of probability, we have $f_X \ge 0$ for all $x \in \mathbb{R}$ and $\sum_{i} f_X (x_i) =1 $.

\begin{example}
    We toss a fair coin two times. Let $X$ represent the number of heads we get. Then CDF of $X$ is,
    \[f_X(x) =
        \begin{cases}
            1/4 \qquad &x=0\\
            1/2 \qquad &x=1 \\
            1/4 \qquad &x=2\\
            0 \qquad &\text{otherwise}
        \end{cases}
    \]
\end{example}

Moreover, for any set of real numbers, $S$, we have
\[ P (X \in S) = \sum_{x \in S} f_X(x)\]
Since all $\{X = x \}$  are disjoint.\\
\par
We can apply similar rules to continuous r.vs,
\begin{definition}
    If $X$ is continuous, we can represent the probability distribution of $X$ with,
    \[ P(a < X < b) = \int_{a}^{b} f_X(x) dx \]
    Function  $f_X$ is called \textbf{Probability Density Function} or PDF as shortly.
\end{definition}

\begin{definition}
    CDF is related to PMF and PDF. For discrete r.v,
    \[F_X(x) = P(X \le x) = \sum_{x_i \le x} f_X(x_i)\]
    And for continuous r.vs,
    \[F_X(x)= \int_{-\infty}^x f_X(x)dx \]
    And $f_X(x) = F_X^{'}(x)$ for for all differentiable points $x$.\\
\end{definition}

\section{Important Random Variables and their distribution}
\begin{definition}
    If $X$ has distribution $A$, we write
    \[ X \sim A\]
    Usually $A$ depends on some fixed numbers to define properly, we call them \alert{parameters}. For example, the distribution
    \textbf{Bernoulli}$(p)$ has parameter $p$. We show parameters in PMF and CDF as,
    \[ f(x; parameters) \quad \text{and} \quad F(x; parameters) \]

\end{definition}
There are some specific examples of r.v. that are very useful in practical applications. We will show most important ones, and briefly
discuss them. In later chapters, we will learn more about them. Note that we will write the notation with the name of the distribution.

\subsubsection{Degenerate distribution or Point mass distribution: $X \sim \delta_a$}
Consider tossing coin or dice where all the sides show the same value. The PMF is ,
\[f_X(x; \delta_a) = 1 \quad \text{for} \ x = a\]
\subsubsection{Discrete Uniform distribution}
This distribution is the one of the most known ones. When there are finitely many values and each of them have the same probability, then
$p = \frac{1}{n}$. Simple coin tossing, dice rolling are prime example of these. The PMF is,
\[f_X(x) = \frac{1}{n}\]
Where $x \in \{1,2,...,n \}$. Nothing new here.
for other cases, $f_X(x) = 0$.
\subsubsection*{Bernoulli distribution}
Bernoulli($p$) describes ``Yes or No'' type of experiments such as coin flipping. Therefore, $P(X = 1) = p$, $P(X = 0) = 1-p$. We can
also calculate PMF,
\[f_X(x; p) = p^x(1-p)^{1-x} \quad \text{for} \ x \in \{0,1\}\]

\subsubsection*{Binomial distribution}
Binomial($n,p$) is the generalized form of Bernoulli distribution. Similar to Bernoulli, this distribution describes ``Yes or no''
type of experiments, but for $n$ times of tries e.g tossing a coin $n$ times. Assuming tries are independent of each other, we can show PMF as,
\[f_X(x; n,p) = \binom{n}{x}p^x(1-p)^{n-x} \quad \text{for} \ x \in \{0,1,...,n\}\]
Notice that Binomial$(1,p)$ = Bernoulli$(p)$.

\subsubsection*{Geometric distribution}
Geom($p$) is also specIID with Bernoulli. The geometric distribution describes the probability of the first occurrence of success
requires after $x$ independent trials e.g getting the first head after $x$ tosses. The PMF is,
\[ f_{X}(x; p) = (1-p)^{x-1}p \quad \text{for} \ x \ge 1 \]

\subsubsection*{Poisson distribution}
Poisson($\lambda$) is mainly used for counts of events like photons hitting a detector in a time interval, number of car accidents, students
achieving a low and high mark on exam, or number of pieces of chewing gum on a tile of a sidewalk. its PMF is,
\[f_X(x; \lambda) = e^{-\lambda} \frac{\lambda^{x}}{x!} \quad \text{for} \ x \ge 0 \]
Usually, $\lambda = rt$ where $r$ is average rate the events occur and $t$ is the time interval. The r.v $X$ represents the number of events.

\subsubsection*{Unfiorm distribution}
The PDF of $X \sim$ Uniform($a,b$) is defined as,
\[f_X(x; a,b)= \frac{1}{b-a} \quad \text{for} \ x \in [a,b]\]

\subsubsection*{Normal (Gaussian) distribution: $X \sim N(\mu, \sigma^2)$}
PDF is defined as,
\[f_X(x; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}}  e^{- \frac{(x- \mu)^2}{2\sigma^2}} \quad \text{for} \ x \in \RR \]

We will learn about $\mu$ and $\sigma$ in later chapters.

We define  \alert{standart Normal distribution} as $N(0,1)$, r.v as $Z$. This specific distribution is very important, so much that we
show its PDF and CDF with new notation, namely $\phi(z)$ and $\Phi(z)$ respectively. There is no closed form expression for $\Phi(z)$.
It can be shown that we can show any normal  probabilities we want with $\Phi(z)$.
\subsubsection*{Exponential distribution}

$X \sim$ Exp($\lambda$) distribution is continous analogue of the geometric distribution.
We define its CDF as ,
\[f_X(x; \lambda) = \lambda e^{-x \lambda}\]
\subsubsection*{Gamma distribution}
$X \sim$ Gamma($\alpha,\beta$), we start with,
\begin{definition}
    For $\alpha > 0$, we define \alert{Gamma function} as,
    \[\Gamma(\alpha) = \int_{0}^{\infty} t^{\alpha-1} e^{-t}dt\]
    It is generalized form of more simple and specific version,
    \[ \Gamma(n) = (n-1)! \quad  \ n \in \mathbb{N} \]
\end{definition}
We define PDF of $x$ as,
\[f_X(x; \alpha, \beta) =  \frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x/ \beta} \quad x > 0\]
Notice that $\operatorname{Gamma}(1, \beta) =\operatorname{Exp}( \beta)$. That is, exponentional distribution is a specific case of gamma
distribution.

\section{Multivariate Distributions}
\begin{definition}
    Let $X_1,X_2,..,X_n$ be r.vs. We call $X = \{ X_1, X_2, ... ,X_n\}$ \alert{a random vector}.
\end{definition}
\begin{definition}
    If r.vs $X_1,X_2,...,X_n$ are independent an have the same marginal distribution with CDF $F$, we define these r.vs
    as \alert{independent and identically distributed}, shortly IID, with notation,
    \[X_1,...,X_n \sim F\]
    similarly, we show the PDF the same way. IID property is very important in statistical field.
\end{definition}
We can apply multivariate CDF as
\begin{definition}
    For $n$  r.v $\{ X_1,X_2,..,X_n \}$, the multivariate CDF $F_{X_1,X_2,...,X_n}$ is given by,
    \[F_{X_1,X_2,...,X_n}(x_1,x_2,...,x_n) = P(X_1 \le x_1,...,X_n \le x_n) \]
\end{definition}
There is nothing fancy here, actually. We simply redefine CDF in general sense for $n$ r.vs.
\par
Similarly, we can define multivariate PMF as,
\begin{definition} For random vector $X$,, the multivariate PMF  $f_{X_1,X_2,...,X_n}$ is given by,
    \[ f_{X_1,X_2,...,X_n}(x_1,x_2,...,x_n) = P(X_1=x_1 ,...,  \ X_n = x_n) \]
    This is generalized form of PMF

\end{definition}
Similarly, we define,
\begin{definition}
    We know that CDF and PDF are related by derivative. Then,
    For random vector $X$, the multivariate PDF $f_{X_1,..,x_N}$ is given by,
    \[f_{X_1,X_2,...,X_n}(x_1,x_2,...,x_n) = \frac{{\partial}^nF_{X_1,..,X_n}(x_1,...,x_n)}{\partial x_1\partial x_2...\partial x_n}\]
\end{definition}

The Properties and theorems are similar, but are generalized for $n$ r.vs.

\section{Marginal Distribution}
If more than one variable is defined in an experiment, it is important to distinguish between the multivariate probability of
$(X_1,X_2,..,X_n)$ and individual probability distributions of $X_1,X_2,..,X_n$\\

Formally, \textbf{Marginal distribution is the probability of a single event (or r.v) occuring, independent of other events}. Therefore
implementing marginal distributions are rather easy. In multivariate distributions, we redefine the needed variable as a ``constant'' and
work with other variables only.
\begin{definition}
    If $X$ is a random vector with PMF $f_{X_1,X_2,..,X_n}$, then we define marginal distribution as,
    \[f_{X_1}= P(X_1 = x_1)= \sum_{x_1 \ constant} P(X_1=x_1,..,X_n=x_n)= \sum_{x_1\ constant} f_{X_1,X_2,..,X_n}(x_1,x_2,...,x_n)\]
\end{definition}
Similarly,
\begin{definition}
    We define marginal PDF as ,
    \[f_{X_i}(x_i) =  \int \int \int ... \int f(x_1,x_2,..,x_n)dx_1..dx_{i-1}dx_{i+1}...dx_n \]
\end{definition}
Similarly, CDF follows the same rule. $F_X(x)= F(x,a,b,c,...)$. \\
\textbf{Remark:} Marginality and conditionality are not the same thing. They look similiar, but their definitions are subtly different.
\par
%------------------------
\section{Independence}
Similar to events, r.vs also can be independent,
\begin{definition}
    Two r.vs $X$ and $Y$ are \textbf{independent} if, for every $A$ and $B$,
    \[P(X \in A, Y \in B)= P(X \in A)P(Y \in B)\]
    The definition persists for multivariate distributions.
\end{definition}
To check Independence, we need to check the above question for every subsets $A,B$. Additionally, we have the theorem,
\begin{theorem}
    Let $X$ and $Y$ have PMF $f_{X_y}$. THen $X$ and $Y$ are independent only and only if ,
    \[f_{X,Y}(x,y)=f_X(x)f_Y(y) \]
    The definiton persists for multivariate distributions.
\end{theorem}

%--------------------------
\section{Conditioning}
Similar to events, r.v $X$ can also have conditional distributions given that we have $Y=y$. We show the conditionality with,
\begin{definition}
    We can show conditional distribution of $X$ respect to $Y$ with,
    \[P(X=x| Y=y) = \frac{ P(X=x,Y=y)}{P(Y=y)} \]
\end{definition}
Moreover we can also define \textbf{conditional PMF} as,
\begin{definition}
    PMF of $X$ conditional respect to $Y$ can be written as
    \[ f_{X|Y}(x|y)= \frac{f_{X,Y}(x,y)}{f_Y(y)}\]
\end{definition}

%---------------------
\section{Transformations of a Random Variable}
In some applications, we really are interested in distributions of some function of $X$. We call this concept \textbf{Transformation of X}.
\begin{definition}
    Let $X$ be r.v with PDF/PMF $f_X$ and CDF $F_X$. Let $Y=r(X)$ i.e $Y=X^2$ or $Y = \ln X$. We call $Y=r(X)$ \textbf{transformation of x}. \\
    \newline
    If $Y$ is discrete, PMF is given by,
    \[f_Y(y)= P(Y=y) =  P( r(X) = y) = P( \{x \ : \ r(x) = y\})= P( X \in r^{-1}(y)) \]
    \newline
    If $Y$ is continuous, we first calculate CDF and find derivative of it.
    \begin{align*}
        F_Y(y) &= P( Y \le y) = P(r(X) < y) \\
        &= P(\{x \ : \ r(x) \le y \}) = P( A_y)    \\
        &= \int_{A_y} f_X(x)dx
    \end{align*}
    And the last step, $f_Y(y) = F^{'}(y)$.
\end{definition}
We can also generalize this concepts for Multivariate distributions, which we just increase dimensions we work with (too lazy, add this later).
