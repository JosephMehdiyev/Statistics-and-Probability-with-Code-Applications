\chapter{Probability}

\section{Set Theory}
Set Theory is well discussed and studied theory across the topics of the mathematics, a great amoount of books across the field starts with
the study of the sets. Therefore, I won't really write any notes about this here other than the important definitions.
\begin{definition}{Event and Sample Space}
    \alert{Sample Space}, as name suggests, is the set of all the possible outcomes of an experiment. The events are subsets of the sample space.
\end{definition}

\section{Definition of Probability}
Although there are multiple interpretations of probability, the soon-to-be-defined axioms rigorously define the explain for further exploration.
\begin{definition}{Probability Function}
    Let $A$ be a event such that $A \subset \Omega$. Then, \alert{Probability of A}, written as $P(A)$, is a function such that
    \begin{enumerate}
        \item $1 \ge P(a) \ge 0$ \\
        \item $P(\Omega) = 1$\\
        \item If $\{ A_i\}$ are disjoint events of $\Omega$, then $P(A_1 \cup A_2 \cup \ldots ) = \sum_{i=1}^{\infty} P(A_i)$
    \end{enumerate}
\end{definition}

First and Second axioms are self-explainatory. Third axiom states the probabilities of two events are independent if the events themselves
are independent.

\begin{lemma} For  events $A$ and $B$,
    \[ P \left(A \cup B\right)= P(A)+P(B)-P(A \cap B) \]

    \begin{proof}
        We can rewrite $A \cup B$ as union of $A \setminus B$, $B \setminus A$, and $A \cap B$, since these are the slices of the thing we want
        to begin with. Moreover, these slices are disjoint, therefore we can apply our third axiom ($P$ is additive):
        $$
        \begin{aligned}
            P \left(A \cup B\right) &= P \bigl( (A \setminus B) \cup (B \setminus A) \cup (A \cap B) \bigr) \\
            &= P(A \setminus B) + P( B \setminus A) + P(A \cap B)  \\
            &= P(A \setminus B) + P( A \cap B)+ P( B \setminus A) + P(A \cap B) - P(A \cap B)  \\
            &= P(A)+P(B)-P(A \cap B)
        \end{aligned}
        $$
    \end{proof}
\end{lemma}

\section{Independent Events}
\begin{definition}
    Two events $A$ and $B$ are \textbf{independent}  if
    \[ P(AB) = P(A)P(B) \]
\end{definition}
\par

\begin{example}
    Let $A = \{ 2,4,6 \}$, $B = \{ 1,2,3,4 \}$. Since $P(A)P(B) = P(AB)$, they are independent.
\end{example}
\begin{example}
    Let $A = \{2,4,6 \}$, $B = \{2,4,5 \}$. Since $P(A)P(B) \neq P(AB)$, they are dependent. \\
\end{example}
Note that even though $AB  \neq \emptyset$, in above examples, the result is not the same. The independence merely shows that another
event can't change other event's probability, even though intuitively it makes no sense.

\section{Conditional Probability}
Conditional Probability, as the name implies, is the probability of an event with a condition. More precisely, \textbf{Conditional
Probability}  is the probability of an event $A$, given that another event $B$ is already occurred. In such probability, the sample space
is reduced to $B$'s, while we want to find probability of $A$ from $B$'s space (Which increases of probability of $A$, since sample space
is also reduced). We can show this neatly in venn diagram:

\begin{example}
    If we tossed a six sided dice one time, and we rolled an even number $B$, what is the probability of getting number $2$, event $A$?

    Since the first toss' result is already happened, we know that  $\Omega_{reduced}=\{2,4,6\}$ and  $A = \{2\}$, then
    $P(A)_{\Omega_{reduced}}=\frac{1}{3}$.
\end{example}

If there wasn't any condition, the probability of getting $2$ would be $\frac{1}{6}$. Simply, in a simple probability we defined a new
condition and sort of updated our measurement to $\frac{1}{3}$. This is an important idea in Probability and Statistics, which we will
revisit shortly in \textbf{Bayes' Rule}
\begin{definition}[Conditional Probability]
    We can show the conditional probability of $A$ given $B$ as:
    \[ P(A | B) = \frac{P(AB)}{P(B)} \qquad \text{for}\ P(B) \neq 0 \]
    Note that $P(A | B) \neq P(B | A)$.
\end{definition}

\section{Bayes' Theorem}

\begin{theorem}[Simplified Bayes' Theorem]
    $$ P(A|B) = \frac{P(A)P(B|A)}{P(B)} $$

    \begin{proof}
        We apply the definition of conditional probability twice:
        $$ P(A|B) = \frac{P(A \cap B)}{P(B)} \qquad \land \qquad  P(B|A) = \frac{P(B \cap A)}{P(A)}$$
        Using above properties directly gives our theorem.
    \end{proof}
\end{theorem}

\begin{theorem}[Law of Total Probability] Let $A_1,A_2,...,A_n$ be partition of $\Omega$. Then for any event $B$,
    $$P(B)= \sum_{i=1}^n P(B|A_i)P(A_i)$$

    \begin{proof}
        Let $C_i=A_iB$. Then we know that $C_1,C_2,...,C_n$ are the partition of $B$. Therefore using the partition property,
        $$ P(B)= \sum_{i=1}^n P(C_i) = \sum_{i=1}^n P(A_i B) =\sum_{i=1}^n P(B|A_i)P(A_i) $$
    \end{proof}
\end{theorem}

\par
This theorem becomes very handy in practical situations. Moreover, with the help of this theorem we can generalize our Bayes' Theorem,

\begin{theorem}[Bayes' Theorem] Let $A_1,A_2,..,A_n$ be a partition of $\Omega$ such that $P(A_i) > 0$. For $P(B) \neq 0$ and for any $i=1,2,...,n$,
    $$ P(A_i|B) = \frac{P(A_i) P(B|A_i)}{P(B)} =  \frac{P(A_i)  P(B|A_i)}{\sum_{i=1}^n P(B|A_i)P(A_i) }$$
\end{theorem}
