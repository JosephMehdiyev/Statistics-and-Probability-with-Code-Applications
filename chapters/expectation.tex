\chapter{Expectations and Invariance}
\section{Expectation of a Random Variable}
The distribution of $X$ contains all the probabilistic data we need about $X$. However, we need additional tools to describe these data more cleanly. 
\par
One of these tools is \textbf{Expectation}, or \textbf{Expected Value} or \textbf{Mean} of $X$.
\begin{definition}
    The \textbf{expected value} of $X$ is defined as,
    \[\mathbb{E}(X)= \begin{cases}
        \sum xf(x)& \text{if $X$ is discrete} \\
        \int xf(x)dx &\text{if $X$ is continous}
    \end{cases}\]
    If expected value is infinite, we say that expected value of $X$ doesn't exist.\\
    We can also combinte both the notations into a whole generalized equation with a notation,
    \[\mathbb{E}(X) = \int x dF(x)= \mu = \mu_X\]
    We have discussed that $dF(x) = f(X)$. in the second chapter.
\end{definition}
By definition of probability, sum of all $f(x)$ is is simply $1$. Then, the above equation is weighted mean of $X$, which is what we wanted to convey.

\begin{example}
    Suppose that we have  a discrete R.V $X$ to describe the probability of getting heads from tossing a coin $3$ times.  Let  c.d.f of $X$ be $f$. Then,
    \[X = \begin{cases}
        &f(0) = 1/8\\
        &f(1) = 3/8 \\
        &f(2) = 3/8 \\
        &f(3) = 1/8
    \end{cases}\]
    Let's use our above formula to calculate $\E(X)$,
    \[\E(X) =  \frac{1}{8} \cdot 0 + \frac{3}{8} \cdot 1 + \frac{3}{8} \cdot 2 + \frac{1}{8} \cdot 3 = 1.5\]
    This number shows that if we repeat our experiment for a very long time, the mean of the heads we got woud be  ( or approach to) $1.5$.
    \\Observe that weighted mean is equivalent to arithmetic mean. Because gettings $X=2$ is simply getting $X=1$ two times.
\end{example}
\par
\vspace{10cm}
But, what if $Y= g(X)$ and we want to compute $E(Y)$? We have a theorem for that,
\begin{theorem}[Law of the Unconscious Statistician]
    Let $Y = g(X)$. Then,
    \[\mathbb{E}(Y)= \E (g(X))= \int g(x) dF_X(x)\]
       The general proof of this theorem is out of the scope of this book. Comparing this to original expectation equation, we can see that the only thing that changes is $g(x)$ and $x$, which intuitively makes sense if you think about it. In transformations, probabiltiies remains unchanged, while the result of probabiltiies gets transformed by a function.
       \\ Moreover, for a special case $g(x) = I_A(x)$, where $I_A(x) \in \{ 0,1\}$ depending on $x \in A$, then,
       \[\E (I_A(X)) = \int I_A(x) d F_X(x) = \int_A dF_X(x) = \mathbb{P} (X \in A)\]
       This means that probability is special case of expectation, which makes sense, considering probability itself is some average by definition.
\end{theorem}
\begin{definition}
We call  $n$-th \textbf{ raw moment} of $X$ as 
\[ \mu_n = \E (X^n) = \int x^n dF_X(x)\] 
If $ \E ( |X^k|)$ is infinite, then $k^{th}$ moment do not exist.\\
We also define \textbf{ $k$-th central moment} as moments about its mean $\mu$ i.e $\E [(X - \mu)^k   ]$. Additionally, \textbf{ $k$-th standardized moments} as $\frac{\E [ (X - \mu)^n]}{\sigma^n}$.
\par
The $1$st moment, the $2$nd central moment, $3$rd  and $4$rd standarized moments are called mean (expected value), \textbf{variance} ,\textbf{skewness} and \textbf{kurtosis} in order. We will learn more about them in later chapters.

\par 
The moments are very useful and practical. Although there  are infinitely many moments, only smaller ones are important for practical purposes. We already know the first moment and its significance. 
\end{definition}
\section{Variance}
We have dicussed about the expectation, a way of showing a property of a distribution. However, the expectation alone doesn't convey much. We have another tool called `\textbf{Variance}'.
Variance, in layman terms, describes how value of random variable varies are spread in the graph. Or in other terms, the distance between the expectation value. 
\par

We can define variance ass,
\begin{definition}
    Let $X$ be a R.V with mean $\mu = \E(X)$. The Variance of $X$, denoted as $\V(X)$ or  Var$(X)$ or $\sigma ^2$ is the $2^{nd}$ central moment and is defined by,
    \[ \sigma^2 = \E [(X- \mu)^2] = \int (x -\mu)^2 dF(x)\]
    
    We also define \textbf{standart deviation} as sd$(X) = \sqrt{\sigma^2}= \sigma$.
\end{definition}
The standart deviation and variance convey the same information. They both represent the spread of our data. 
The difference between them is purely mathematical. The variance is more useful in mathematical applications, where standart deviation is very intuitive and practical. 
\href{https://www.mathsisfun.com/data/standard-deviation.html}{mathisfun} explains it very well.
\par
Calculating variance directly can be complicated  and tedious directly sometimes. We can derive a theorem from the original definition for practical purposes.
\begin{theorem}
    Let $X$ be a random variable. Then,
    \[\sigma^2 = E([ (X-\mu)^2]) = E(X^2) - \mu^2\]
    \begin{proof}
        It is derived directly by algebraic manipulation and basic calculus,
        \begin{align*}
            \sigma^2 &= \int (x- \mu)^2 dF(x)  \\
                     &= \int x^2 dF(x) -2\mu \int x dF(x) + \mu^2 \int dF(x) \\
                     &= \int x^2 dF(x) - \mu^2 \\
                     &= \E(X^2) - \mu^2
        \end{align*}
    \end{proof}
\end{theorem}

\section{Covariance and Corelation}
Ley $X$ and $Y$ be R.V.  \textbf{ Covariance} and \textbf{ Corelation} describes the linear relationship between $X$ and $Y$.
\begin{definition}
    If $X$ and $Y$ are R.V with mean $\mu_X$, $\mu_Y$ and standart deviations $\sigma_X$, $\sigma_Y$, we define \textbf{covariance} as,
    \[\mathrm{Cov}(X,Y) = \E \bigg((X- \mu_X)(Y - \mu_Y) \bigg)\]
    and \textbf{corelation} as,
    \[\rho = \rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X\sigma_Y}\]
    Notice that $\mathrm{Cov}(X,X) = \mathbb{V}(X)$ and $\rho_{X,X} = 1$.
\end{definition}
Similiar to variance, calculatig covariance can be tedious. We can derive a better formula by simple algebraic manipulations,
\begin{theorem}
    For all random variables with non-infinite means, we have
    \[\mathrm{Cov}{(X,Y) = \E(XY)-\E(X)\E(Y)}\]
    \begin{proof}
        Similar to Variance one, we have,
        \begin{align*}
                    \mathrm{Cov(X,Y)}  &= \E \bigg( (X- \mu_X)(Y - \mu_Y) \bigg)\\
                                        &= \E ( XY- X\mu_Y-Y\mu_X+\mu_X\mu_Y ) \\
                                        &= \E(XY) - \mu_Y\E(X) - \mu_X\E(Y) + \mu_X\mu_Y \\ 
                                        &= \E(XY) - \E(X)\E(Y)
        \end{align*}
    \end{proof}
\end{theorem}
\begin{theorem}
    For all random variables with non-infinite means, we have,
    \[ -1 \le \rho_{X,Y} \le 1\]
    \begin{proof}
        It is direct consequence of Cauchy-Schwarz inequality.
    \end{proof}
\end{theorem}
