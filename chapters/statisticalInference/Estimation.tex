\chapter{Estimation}
Through the last chapter, we have talked about multiple probability distributions and their functions such as c.d.f, p.d.f, and p.m.f. 
However, we have assumed that we already knew the distribution and its properties. In practical world, it is not the case.
We may try to find the average salary of the country, the fatality rate of a virus, and so on.
Statistical inference, in shortly, is study of using the information sample we are given to deduce the characteristics of a population.
Since majority of population is defined with \textbf{paramateres}, our investigation is mainly on finding, or estimating such paramteres.



\section{Introduction}
Majority of times we work with multiple parameters. However, we might be interested on only one of them. Therefore,  we call the parameter of our interest to be \textbf{target parameter}.
\par
    Suppose we want to estimate $\mu$ of some experiment. We could give our estimate in two forms: $\textbf{point estimate and interval estimate}$. As name implies, that point estimate is a single value, however interval estimate is an interval.

\begin{example}
    Suppose we want to estimate the average score  $\mu$ the students will get from SAT score this year. We could use just a number $1240$, a \textbf{point estimate}, or an interval $(1200,1260)$, a \textbf{interval estimate}.
\end{example}

Examples for such estimators can be
\[\overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i \]
Which esimates the mean $\mu$. Notice that $\E(\overline{Y}) = \mu$ and,
\[ S^2 = \frac{1}{n-1} \biggl(  \sum_{i = 1}^n  (X_i - \overline{X})^2 \biggr) \]
Which estimates the variance $\sigma^2$. Notice that $\E(S^2) = \sigma^2$.\newline
There can be multiple estimators estimating the same parameter. It is logical, since not all estimators are ideal for practical purposes because of quality of estimator.
\section{Point Estimation}
By convention we write the estimate of $\theta$ as $\widehat{\theta}$. Since $\theta$ is constant and by definition $\widehat{\theta}$ is a function, $\widehat{\theta}$ is a r.v. Remark that funtion of r.vs is a r.v. In more mathematical way,
\begin{definition}
    Let $X_1,..X_n \sim F$ be i.i.d. A point estimator $\widehat{\theta}$ is defined as,
    \[\widehat{\theta} = g(X_1, \ldots,X_n)\]
    We also define a very useful variable \textbf{bias} as,
    \[\op{bias}(\widehat{\theta}, \theta) = \op{E}(\widehat{\theta}) - \theta\]
    Here, $\theta$ is our target parameter, $\widehat{\theta}$ is the function we use to estimate our target, or the estimator. We usually write $\op{bias}(\widehat{\theta},\theta) = \op{bias}(\widehat{\theta})$ 
    \\
    Bias, in a literal sense, tells us the bias of the estimator we use. That is, the error that we may find when we estimate our parameter. We say that $\widehat{\theta}$ is \textbf{unbiased} if,
    \[\op{bias} (\widehat{\theta}) = 0 \Rightarrow \E(\widehat{\theta}) = \theta \]
\end{definition}
We know that $\t$ is a r.v. We call this r.v's distribution as $\textbf{sampling distribution}$. We also define,
\textbf{standart error of} $\t$ or standard deviation,
\[\sigma_{\t}= \sqrt{\V(\t)}\]
It is logical to think that the estimator should converge (with more samples) to its target value, we define such property as,
\begin{definition}
    If a point estimator $\t$ converges to $\theta$, we call that $\t$ is \textbf{consistent}
\end{definition}

\par
With bias alone, we can't characterize the quality of the estimator. Because the values of $\t$ may be far away than real value $\theta$, but still be $\E(\t) = \theta$. 
Therefore, we also have to measure the variance in some way.
\\
For such thing, we already have a tool,
\begin{definition} 
    \textbf{The mean square error} is defined as,
    \[\op{MSE}(\t) = \E([\t - \theta]^2) \]
    in similiar fashion to the Variance definition, we can rewrite this equation as,
    \[\op{MSE}(\t) =\op{bias}^2(\t) + \V(\t)\]
\end{definition}
MSE is function of both its variance and bias, hence it is a better way of showing the quality of the estimator.

\begin{example}
    Let $X_1, \ldots ,X_n \sim \op{Bernoulli}(p)$. Let $\widehat{p} = n^{-1}\sum_{i=1}^n X_i$. We already know that $\E(X) = p$ and $\V(X) = p(1-p)$.  Our estimator is unbiased since,
    \[ \E( \widehat{p}) = \frac{1}{n} \sum \E(X_i) = p\]
    Moreover, the estimator's variance is, 
    \[ \sigma_{\widehat{p}}^2 = \E(\widehat{p}^2) - \E(\widehat{p})^2 = p - p^2 \]
\end{example}
We can intutively guess some estimators that could be effective for our purposes. But for many complex problems, it is not the case. We will learn new methods to calculate estimators in later chapters.
\section{Confidence Intervals}
Let's assume we are a scientist that want to measure the mean of the specific kind of mice's weight.
It is unrealistic to measure \textbf{all of the mice}, hence we catch a small amount of them, probably in hundreds, measure them and gather the data in a datasheet. We \textbf{bootstrap} (we will learn what that term is in later chapters) the sample data, and find the sample mean. Now, we repeat the bootstrapping process thousands of times, which now we have a sample mean data.
\\
Now, let's find numbers $a,b$ such that $95\%$ of our sample mean data resides in interval $[a,b]$. That is what \textbf{confidence interval} basically is.
\begin{definition}
    Let $X$ be a random vector. The $1 - \alpha$ \textbf{confidence interval} for a parameter $\theta$ is an interval $[a,b]$ and $a = a(X)$, $b = b(X)$ functions such that,
    \[ P(a \le \theta \le b) \ge  1 - \alpha \]
    Note that $\theta$ is unknown constant value, while  $a$ and $b$ are random variables. 
   \\
   Taking the above example, $\alpha = 0.05$, which is a mathematical standard number used majority of time. $1 - \alpha$ is called \textbf{confidence coefficient}. We also call \textbf{lower and upper confidence limits} to $a$ and $b$, sometimes also donated as $\theta_{L}$ and $\theta_{U}$.
\end{definition}
It is also possible to form \textit{one sided confidence interval}, i.e,
\[ P( \theta_{L} \le \theta ) \ge 1 - \alpha  \qquad \text{or} \qquad P( \theta \le \theta_{U}) \ge 1 - \alpha\]
The confidence intervals may be \textbf{closed or open}. For our purpose they are indifferent. 


