\chapter{Statistical Models and Statistical Inference}
Through the last chapter, we have talked about multiple probability distributions and their functions such as c.d.f, p.d.f, and p.m.f. 
However, we have assumed that we already knew the distribution and its properties. In practical world, it is not the case.
We may try to find the average salary of the country, the fatality rate of a virus, and so on.
Statistical inference, in shortly, is study of using the information sample we are given to deduce the characteristics of a population.
Since majority of population is defined with \textbf{paramateres}, our investigation is mainly on finding, or estimating such paramteres.



\section{Model and Inference}
Assume that $ X \sim N(0,1)$ and $Y \sim N(0,2)$. The $X$ and $Y$ don't have the same probability characteristics, more cleanly, they don'thave the same distribution. However, we generalize them in a ``group'' called \textit{normal distribution}. That is, there are infinitely many distribution with $N(\mu, \sigma^2)$, and to convey them in a more general way, we use \textbf{statistical models}
\begin{definition}
    \textbf{Statistical Model} is a set of distribution such as,
    \[ \mathfrak{F} = \biggl \{ f(x; \theta ): \theta \in \Theta \biggr \} \]
    where $\Theta$ is a \textbf{parameter space}.\\
    There are also \textbf{nonparametric} $\mathfrak{F}$. 
\end{definition}
In a parametric model, if we are interested in only one parameter $\theta$, we call it \textbf{target parameter} or \textbf{estimand}.  The process of getting the estimand is called \textbf{Statistical Inference}. Majority of inferental problems are divided into three types: \textbf{estimation, confidence sets, hypothesis testing}.


\section{Point Estimation}
By convention we write the estimate of $\theta$ as $\widehat{\theta}$. Since $\theta$ is constant and by definition $\widehat{\theta}$ is a function, $\widehat{\theta}$ is a r.v. Remark that funtion of r.vs is a r.v. In more mathematical way,
\begin{definition}
    Let $X_1,..X_n \sim F$ be i.i.d. A point estimator $\widehat{\theta}$ is defined as,
    \[\widehat{\theta} = g(X_1, \ldots,X_n)\]
    We also define a very useful variable \textbf{bias} as,
    \[\op{bias}(\widehat{\theta}, \theta) = \op{E}(\widehat{\theta}) - \theta\]
    Here, $\theta$ is our target parameter, $\widehat{\theta}$ is the function we use to estimate our target, or the estimator. We usually write $\op{bias}(\widehat{\theta},\theta) = \op{bias}(\widehat{\theta})$ 
    \\
    Bias, in a literal sense, tells us the bias of the estimator we use. That is, the error that we may find when we estimate our parameter. We say that $\widehat{\theta}$ is \textbf{unbiased} if,
    \[\op{bias} (\widehat{\theta}) = 0 \Rightarrow \E(\widehat{\theta}) = \theta \]
\end{definition}
We know that $\t$ is a r.v. We call this r.v's distribution as $\textbf{sampling distribution}$. We also define,
\textbf{standard error of} $\t$ or standard deviation,
\[\se= \sqrt{\V(\t)}\]
It is logical to think that the estimator should converge (with more samples) to its target value, we define such property as,
\begin{definition}
    If a point estimator $\t$ converges to $\theta$, we call that $\t$ is \textbf{consistent}
\end{definition}

\par
With bias alone, we can't characterize the quality of the estimator. Because the values of $\t$ may be far away than real value $\theta$, but still be $\E(\t) = \theta$. 
Therefore, we also have to measure the variance in some way.
\\
For such thing, we already have a tool,
\begin{definition} 
    \textbf{The mean square error} is defined as,
    \[\op{MSE}(\t) = \E([\t - \theta]^2) \]
    in similiar fashion to the Variance definition, we can rewrite this equation as,
    \[\op{MSE}(\t) =\op{bias}^2(\t) + \V(\t)\]
\end{definition}
MSE is function of both its variance and bias, hence it is a better way of showing the quality of the estimator.

\begin{example}
    Let $X_1, \ldots ,X_n \sim \op{Bernoulli}(p)$. Let $\widehat{p} = n^{-1}\sum_{i=1}^n X_i$. We already know that $\E(X) = p$ and $\V(X) = p(1-p)$.  Our estimator is unbiased since,
    \[ \E( \widehat{p}) = \frac{1}{n} \sum \E(X_i) = p\]
    Moreover, the estimator's variance is, 
    \[ \V(\widehat{p}) = \E(\widehat{p}^2) - \E(\widehat{p})^2 = p - p^2 \]
\end{example}
We can intutively guess some estimators that could be effective for our purposes. But for many complex problems, it is not the case. We will learn new methods to calculate estimators in later chapters.
\section{Properties of Point Estimation: Efficienty, Consistency, Sufficiency}
\subsection*{Relative Efficiency}
We already know that it is possible to have multiple estimators for one target parameter. 
We even learnt a new definiion, MSE to convey the quality of such estimators.
If we have two unique and unbiased estimators $\t_1$ and $\t_2$, it is logical to pick the estimator that has the lowest variance, since the lower the MSE, more efficient the estimator is. To convey such idea, we use, 
\begin{definition}
    Given two unbaised estimators $\t_1$ and $\t_2$, then \textbf{the efficiency of $\t_1$ relative to $\t_2$}, denoted as $\op{eff}(\t_1,\t_2)$, is defined as,
    \[ \op{eff}(\t_1, \t_2) = \frac{\V(\t_2)}{\V(\t_1)} \]

    Note that  if $\op{t_1:w, t_2}$ is bigger than one, then it is true that $\t_1$ is relatively more efficient than $\t_2$.
\end{definition}
\subsection*{Consistency}
We have already talked about consistency before, we say the estimator is consistent of it converged to the target parameter,
\begin{definition}
        The estimator $\t$ of $\theta$ is consistent if for any positive number $\epsilon$,
    \[ \lim_{n \rightarrow \infty} P(| \t_{n} - \theta| \le \epsilon) = 1 \]
        That is, $\t \stackrel{p}{\longrightarrow} \theta$

\end{definition}
The graph (below) from latter exercises is also consistent, since visually it becomes a straight line where it equals to the target parameter.

Since consistent estimators converge to the target parameter, it is logical to think the variance also converges to $0$. Think in a way that the graph of the estimator has to become straight from long wavy and curvy lines i.e it is direct consequence of convergence (real analysis stuff?). Indeed,
\begin{theorem}
    The unbiased estimator $\t$ of $\theta$ is a consistent estimator if, 
    \[ \lim_{n \rightarrow \infty} \V(\t_{n}) = 0 \]
\end{theorem}




\subsection*{Sufficiency}
We know that the value $\overline{X}$ (average value) is a unbiased estimator for mean $\mu$ of $X$. At this point, we no longer need the sample data to estimate the $\mu$, since we can summarize the information just with the estimator $\overline{X}$.
But, do the $\overline{X}$ retain all the information about $X$?. If it does, we call such estimator \textbf{sufficient}. That is all the sufficiency is for.

We can mathematically convery this property as conditional distribution of our sample data, given the estimator. If the distribution is dependent on our target parameter, it can't be sufficient. In more mathematical way,
\begin{definition}
    A \textbf{statistic} is a function of data (Remark: all estimators are statistic but not all statistic are estimators). A statistic $U = t(X_1,..,X_n)$ of $\theta$ is sufficient if conditional distribution of $X_1,...,X_n$ given $U$ is not dependent on $\theta$.
    \newline
    \newline
    If conditional distribution is dependent on the target parameter, it is intuitive to think the statistic does not contain all the information.
\end{definition}
Sufficiency is useful since it helps us to \textit{assessing information on the  entire population without the need of all the data}.\\
Say you get your grade on an exam and you want to know how well you did compared to your classmates. If you are given a sample mean and variance, you can do this without asking everyone's grades.

\section{Confidence Intervals}
Let's assume we are a scientist that want to measure the mean of the specific kind of mice's weight.
It is unrealistic to measure \textbf{all of the mice}, hence we catch a small amount of them, probably in hundreds, measure them and gather the data in a datasheet. We \textbf{bootstrap} (we will learn what that term is in later chapters) the sample data, and find the sample mean. Now, we repeat the bootstrapping process thousands of times, which now we have a sample mean data.
\\
Now, let's find numbers $a,b$ such that $95\%$ of our sample mean data resides in interval $[a,b]$. That is what \textbf{confidence interval} basically is.
\begin{definition}
    Let $X$ be a random vector. The $1 - \alpha$ \textbf{confidence interval} for a parameter $\theta$ is an interval $[a,b]$ and $a = a(X)$, $b = b(X)$ functions such that,
    \[ P(a \le \theta \le b) \ge  1 - \alpha \]
    Note that $\theta$ is unknown constant value, while  $a$ and $b$ are random variables. 
   \\
   Taking the above example, $\alpha = 0.05$, which is a mathematical standard number used majority of time. $1 - \alpha$ is called \textbf{confidence coefficient}. We also call \textbf{lower and upper confidence limits} to $a$ and $b$, sometimes also donated as $\theta_{L}$ and $\theta_{U}$.
\end{definition}
It is also possible to form \textit{one sided confidence interval}, i.e,
\[ P( \theta_{L} \le \theta ) \ge 1 - \alpha  \qquad \text{or} \qquad P( \theta \le \theta_{U}) \ge 1 - \alpha\]
The confidence intervals may be \textbf{closed or open}. For our purpose they are indifferent. 


\section{Hypothesis Testing}
Please read the Hypothesis chapter instead. \\

\section{References}
\begin{enumerate}
    \item  \url{https://math.stackexchange.com/questions/1767877/in-laymans-terms-what-is-the-difference-between-a-model-and-a-distribution}
    \item \url{https://stats.stackexchange.com/questions/3911/when-are-confidence-intervals-useful}
    \item \url{https://mason.gmu.edu/~alaemmer/bio214/sampling-distributions.pdf}
    \end{enumerate} 
\section{Exercises}
