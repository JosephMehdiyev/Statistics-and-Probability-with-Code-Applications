\chapter{Hypothesis Testing and p-value}
\section{Null and Alternate Hypothesis}
The hypothesis testing is very similar to the scientific method.
Scientists across the different fields use scientific method for their academical purposes.  The observe, formulate a theory, experiment and test the theory. There is a similar method called \textbf{hypothesis testing} for statistical inference. First, we will introduce some notations and definitions,
\begin{definition}
    \textbf{Null Hypothesis}, denoted by $H_0$, is the hypothesis to be tested.\newline
    \textbf{Alternate Hypothesis}, denoted by $H_1$, is the hypothesis contradictory to the null hypothesis. We usually try to support, since this way we could use \textit{proof by contradiction}. \
    If our evidence (data) favors the  alternative hypothesis, we reject the null hypothesis. Formally, we wish to test,
    \[H_0  : \theta  \in \Theta_0 \qquad or \qquad H_1 : \theta \in \Theta_1 \]
    Where $\Theta_0$ and $\Theta_1$ are disjoint sets of parameter space $\Theta$.
\end{definition}
\begin{example}
    Let $X_1,...,X_n \sim N(\mu, \sigma^2)$  with known variance $\sigma^2$ and uknown mean $\mu$. We wish to test the hypothesis,
    \[ H_0: \mu = \mu_0 \qquad or \qquad H_1: \mu  \neq \mu_0 \]
\end{example}
In order to test the hypothesis, it is logical to calculate $\overline{X}$ and compare it with $\mu_0$. It is reasonable to reject $H_0$ if $\overline{X}$ is far away than $\mu_0$. But how much far away exaclty? \newline
\textbf{Rejection or Critical Region} is a set denoted by $R$ to describe ``how far away'' the result can be. If the result is in the set $R$, we reject the hypothesis.
\begin{definition}
    The \textbf{Critical Region} is defined as,
    \[ R = \biggl \{ x : T(x) > c \biggr \} \]
    Here, $c$ is called \textbf{critical value}, and $T$ is a \textbf{test statistic} to help testing our hypothesis.
    The main question in hypothesis testing is find appropriate $T$ and $c$. 
\end{definition}
In above example, $\overline{X}$ is our $T$. We will learn about finding $c$ now.
\newline

A hypothesis in the form of $\theta = \theta_0$ is called \textbf{simple} i.e $\theta$ has only one value, while in the form of $\theta > \theta_0$ or $\theta < \theta_0$ is called \textbf{composite} i.e $\theta$ has multiple values

There are $4$ different possibilities we can conclude from our test. See the table for the brief introduction.\newline
If $H_0$ is true, and we reject it, we call this error \textbf{type I error}. Similarly, if $H_1$ is true and we keep the $H_0$, (or reject $H_1$) we call this error \textbf{type II error}. Summarize this in table,
\begin{center}
\begin{tabular}{|c|c|p{4.1cm}|}
    \hline
     & Retain Null & Reject Null \\
    \hline
    $H_0$ \text{true} & ($1 - \alpha$) & type I error ($\alpha$) signifigance level of the test\\
    \hline
    $H_1$  \text{true} & type II error ($\beta$) &  power of the test ($1 - \beta$)\\
    \hline
\end{tabular}
\end{center}
\begin{definition}
    The $\textbf{power function}$ of a test with critical region $R$ is a function defined as,
    \[ \beta (\theta) = P_{\theta}( X \in R)\]
    that is, $\beta(\theta)$ represents the  probability of rejecting $H_0$ if $\theta \in \Theta_0 \cup \Theta_1$ i.e $H_0$ or $H_1$ true. It is very general\\
    The \textbf{size} of the test is defined by,
    \[ \alpha = \sup_{\theta \in \Theta_{0}} \beta(\theta) \]
    that is, for $\theta$ that $H_0$ is true, $\alpha$ is the worst case scenario probability, representing \textbf{type I error} (if the test is simple).\\
    We also define \textbf{significance level} of the test as $\alpha$  if its size is less than or equal to $\alpha$. That is, upper bound for incorrectly rejecting $H_0$. Significance level is chosen independent of data or the tests, usually as $0.05$. Note in particular that both size and level don't relate to the sample.
\end{definition}

If we are conducting pointwise test, for example $\mu = \mu_0$, then \textbf{significance level, the size of the test, and type I error} coincide, that is they are equal. However, for some special cases they may differ. Similarly, \textbf{power,1- type II error} may also differ.

\section{p-value}
The size of the test and power of the test is not that much related to our sample data. That is where \textbf{p-value} comes in. \\
The p-value is the probability under the null hypothesis of obtaining a real-valued test statistic at least as extreme as the one obtained, more rigirously,
\begin{definition}
    Suppose for every $\alpha \in \{ 0,1 \}$ we have a rejection region $R_{\alpha}$. Then \textbf{p-value} is defined as,
\[ p = \inf \biggl \{  \alpha: T(X) \in R  \biggr \} \]
That is, the p-value is the smallest level at which we can reject $H_0$. We can calculate $p$ with,
\[ p = \sup_{\theta \in \Theta_{0}} P_{\theta}( T(X) \ge T(x)) \]
Notice that the expression contains sample data $x$.
\end{definition}
\section{References}
\begin{enumerate}
    \item \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_test#Definition_of_terms}
    \item \url{https://stats.stackexchange.com/questions/183800/how-to-understand-the-size-of-hypothesis-tests?rq=1}
    \item \url{https://stats.stackexchange.com/questions/299873/differences-between-p-value-level-of-significane-and-size-of-a-test}
    \item \url{https://en.wikipedia.org/w/index.php?title=P-value&oldid=554910098}
\end{enumerate}

