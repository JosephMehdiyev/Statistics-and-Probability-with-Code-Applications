\chapter{Methods of Estimation (Parametric Inference)}
In later chapter we have shortly talked about the point estimation. The estimator $\t$ of a target parameter $\theta$ is a function of random variables of a sample, and therefore it itself is a random variable.
The estimator has its own probability distribution, \textit{sampling distribution}. We already know about \textit{unbaised estimators} i.e $\E(\t) = \theta$ and the \textit{consistent estimator}. In this chapter, we will learn more deeply about the mathematical properties of the point estimators. Additionally, we will learn new methods to derive estimators, since until now we listened our intuition.
\section{Method of Moments}
Until now, we have used our intuiton to find estimators. For example, it is logical to think that $\overline{X}$ would be an ideal estimator for $\mu$ of $X$. However, in practical world we have to generate the parametric estimators more ``mathematically''.
First, we introduce with a new simple definition,
\begin{definition}
    \textbf{k-th sample moment} $\widehat{\alpha}_k$ is moment of sample i.e 
    \[\widehat{\alpha}_k = \frac{1}{n} \sum_{i = 1}^n X_{i}^k \]
\end{definition}
In section 3.1 we talked about \textbf{raw moments}. Raw moments convey the properties of the distribution i.e raw moments are some functions of the desired parameters. The first raw moment is the mean $\mu_1 = \mu$, the second raw moment is expression of variance  $\mu_{2}= \sigma^2+ \mu^2$ and so on.

The idea method of moment is  we can use $\widehat{\alpha}_k$ as good estimator of $\mu_k$, and from $\mu_k$ we can derive expressions for our target parameter.

\begin{example}
Let $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$. We have that $\E(X) = \mu$ and $\E(X^2) = \sigma^2 + \mu^2$.
\begin{align*}
    &\widehat{\alpha}_1 = \widehat{\mu} \\
    &\widehat{\alpha}_2 = \widehat{\mu}^2 + \widehat{\sigma^2}
\end{align*}
Solving the system equation will give us estimators for $\mu$ and $\sigma$.

\end{example}

\section{Method of Maximum Likelihood}
The method of moments are very simple and intuitive, but it is unefficient. We have a better and sophisticated method called 
\textbf{method of maximum likelihood}. There is a great \href{https://www.youtube.com/watch?v=XepXtl9YKwc}{video}  by Josh Starmer that explains the method very well.

Assume that we have a sample data, and we want to estimate parameters of the distribution that describes the sample data. The idea is that we find such estimator that maximaze the \textbf{likelihood} of getting our sample data relative to the parameter.
\begin{definition}
    The \textbf{Likelihood function} is defined as,
    \[ \mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i; \theta) \]
    Also we define \textbf{log-likelihood function} as,
    \[\ell_n( \theta) = \log \mathcal{L}_n(\theta) \]
    At last, we define the \textbf{maximum likelihood estimator} MLE denoted by $\t_n$ as the value of $\theta$ that maximizes $\mathcal{L}_n(\theta)$, or better $\ell_n( \theta)$, since working with logs are easier that multiplicative functions for maximizing.
\end{definition}
We already know that $\theta$ is a unknown constant we want to estimate. The $\mathcal{L}_n(\theta)$ describes the likelihood of each sample data, respect to $\theta$. Since it is intuitive to maximize the likelihood (because the sample data is already happened and should be maximized), it should also estimate our value $\theta$.
\section{Properties of MLE}
Will write later.
