\chapter{Methods of Estimation (Parametric Inference)}
In later chapter we have shortly talked about the point estimation. The estimator $\t$ of a target parameter $\theta$ is a function of random variables of a sample, and therefore it itself is a random variable.
The estimator has its own probability distribution, \textit{sampling distribution}. We already know about \textit{unbaised estimators} i.e $\E(\t) = \theta$ and the \textit{consistent estimator}. In this chapter, we will learn more deeply about the mathematical properties of the point estimators. Additionally, we will learn new methods to derive estimators, since until now we listened our intuition.
\section{Properties of Point Estimation: Efficienty, Consistency, Sufficiency}
\subsection*{Relative Efficiency}
We already know that it is possible to have multiple estimators for one target parameter. 
We even learnt a new definiion, MSE to convey the quality of such estimators.
If we have two unique and unbiased estimators $\t_1$ and $\t_2$, it is logical to pick the estimator that has the lowest variance, since the lower the MSE, more efficient the estimator is. To convey such idea, we use, 
\begin{definition}
    Given two unbaised estimators $\t_1$ and $\t_2$, then \textbf{the efficiency of $\t_1$ relative to $\t_2$}, denoted as $\op{eff}(\t_1,\t_2)$, is defined as,
    \[ \op{eff}(\t_1, \t_2) = \frac{\V(\t_2)}{\V(\t_1)} \]

    Note that  if $\op{t_1:w, t_2}$ is bigger than one, then it is true that $\t_1$ is relatively more efficient than $\t_2$.
\end{definition}
\subsection*{Consistency}
We have already talked about consistency before, we say the estimator is consistent of it converged to the target parameter,
\begin{definition}
        The estimator $\t$ of $\theta$ is consistent if for any positive number $\epsilon$,
    \[ \lim_{n \rightarrow \infty} P(| \t_{n} - \theta| \le \epsilon) = 1 \]
\end{definition}
The graph (below) from latter exercises is also consistent, since visually it becomes a straight line where it equals to the target parameter.

Since consistent estimators converge to the target parameter, it is logical to think the variance also converges to $0$. Think in a way that the graph of the estimator has to become straight from long wavy and curvy lines i.e it is direct consequence of convergence (real analysis stuff?). Indeed,
\begin{theorem}
    The unbiased estimator $\t$ of $\theta$ is a consistent estimator if, 
    \[ \lim_{n \rightarrow \infty} \V(\t_{n}) = 0 \]
\end{theorem}
\textbf{REMARK:} There are multiple concepts of "convergence". In this case, it is \textbf{convergence in probability}. I really should study real analysis huh.

\section{Rao-Blackwell theorem}
\section{Method of Moments}
\section{Method of Maximum Likelihood}
\section{Method of Delta}
\section{Multi-Parameter Models}

