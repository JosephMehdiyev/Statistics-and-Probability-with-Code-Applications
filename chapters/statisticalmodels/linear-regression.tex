\chapter{Linear Regression}
Given the data with random vectors $X,Y$ of the form
\[ (\mathbf{X}, \mathbf{Y}) \sim F_{X,Y}\]
\textbf{Regression} is a method finding a linear function to fit properly our data. This way, we can predict the value of $Y_i$, or estimate in other words.
\section{Simple Linear Regression Model}
\begin{definition}
We can write the simpliest linear regression as follows,

\[Y_i = \beta_0 + \beta_1X_i + \epsilon_i \]
Here, $\epsilon_i$ is called \textbf{error variable, disturbance term, or error term}. 
Error term represents factors other than $x$ that affects $y$.
It is normall assumed  that $\epsilon_i \sim N(0, \sigma^2)$. That is,
\[ \E(\epsilon_i | X_i) = 0 \quad \land \quad \V(\epsilon | X_i) = \sigma^2 \]
We work with conditionalities in linear regression, since the idea of the regression itself is conditional i.e Y depends on X.
\end{definition}

The unknown variables in our model are \textbf{intercept} $\beta_0$, \textbf{slope} $\beta_1$ and our $\epsilon_i$. We can estimate $\epsilon_i$ with \textbf{residual} $\widehat{\epsilon_i}$. 
In practice we would like to minimize the values of,
\begin{definition}
    \textbf{Residual Sums of Squares} or shortly \textbf{r.s.s or RSS},
    which measures how our model works with our data, defined as,
    \[\op{RSS} = \sum_{i=1}^n \widehat{\epsilon_i}^2 \]
We also define \textbf{least square estimates}, that are values 
$\beta_0$ and $\beta_1$ such that minimizes RSS.
We can derive an expression for least square estimates. In later chapters, we will also work with multivariate versions of the same topic.
\begin{theorem}
    The least square estimates are given by,
    \begin{align*} &\widehat{\beta}_1 = \frac{\sum_{i = 1}^n(X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i = 1}^n (X_i - \overline{X})^2} \\
        &\widehat{\beta}_0 = \overline{Y} - \widehat{\beta}_1\overline{X} \\
        &\widehat{\sigma}^2 = \left( \frac{1}{n-2}\right)\sum_{i=1}^n \widehat{\epsilon}^2
\end{align*}
The last estimator is also called \textbf{residual standar error} or shortly RSE.\\
All of these estimators are unbiased.
We usually denote the least square estimators as matrices i.e $\widehat{\beta}^T = (\widehat{\beta_0},\widehat{\beta}_1)^T$, for convenience.
\begin{proof}[Proof Sketch]
    Notice that
    \[\op{RSS} = \sum_{i=1}^n (Y_i - \beta_0 -\beta_1X_i) \]
    Therefore it is enough to minimize the right expression, which is trivial via derivatives, and solving the system equations. We will later see the multivariate proof of the theorem via matrices and such.
\end{proof}
\end{theorem}
\end{definition}
\section{Maximum Likelihood and Least Square Estimators}
Assume that $Y$ is normally distributed. It is very practical, and holds true in practice. The likelihood function is then,
\[ \prod_{i=1}^nf(X_i, Y_i) = \prod_{i=1}^n f_X(X_i) \prod_{i=1}^n f_{Y|X}(Y_i | X_i) = \mathcal{L}_{1} \cdot \mathcal{L}_2\]
We can use maximum likelihood method to estimate our parameters $\beta_1, \beta_2$. $\mathcal{L}_1$ doesn't include our target parameters, so we will treat it as a constant. We wish to maximize $\mathcal{L}_2$. Using the log-likelihood and our normal distribution assumption,
\begin{align*}
    \ell(\beta_0, \beta_1, \sigma) = -n \ln(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n\left(Y_i - (\beta_0 + \beta_1 X_i)\right)^2
\end{align*}
Note that we used the p.d.f of normal distribution and the fact that $\mu = \beta_0 + \beta_1X_i$. To maximize the log-likelihood, we have to maximize $\op{MLE}$ of $(\beta_0, \beta_1)$
\[\op{RSS} = \sum_{i=1}^n\left(Y_i - (\beta_0 + \beta_1 X_i)\right)^2\]
Which is the least square estimator. In fact, we have the following theorem,
\begin{theorem}
    As long as we have the normality assumption, the least square estimator and likelihood estimator are equivalent.
\end{theorem}
\section{Properties of  Least Sqaure Estimators}
We begin with the expectation and variance of least square estimators. Remember that we work with conditionals on $X$.
\begin{theorem}
    The least square estimator matrix $\widehat{\beta}$ is unbiased i.e
    \[\E(\widehat{\beta} | X) = 
    \begin{pmatrix}
        \beta_0\\
        \beta_1
    \end{pmatrix}\]
    \begin{proof}[Proof Sketch]
        We already derived a way of writing least square estimators in the latter chapter, \textbf{Theorem 12.1.1}. Using that fact and $\E(\epsilon | X) = 0$, it is matter of manupilating the expectation until we get our result.
    \end{proof}
\end{theorem}
\begin{theorem}
    Following properties are true:
    \begin{enumerate}
        \item Consistency: $\widehat{\beta} \stackrel{p}{\longrightarrow} \beta$
        \item Asymptotically normal: $\sqrt{n}(\widehat{\beta} - \beta) \stackrel{d}{\longrightarrow} N(0, 1)$
        \item $1-\alpha$ confidence intervals:
            \[ \beta \pm z_{\alpha/2}\op{\widehat{se}(\beta)}\]
    \end{enumerate}
\end{theorem}
\section{Multivariate Regression}
Suppose that, now we have multiple input variables $X_1, \ldots X_k$ i.e 
\[\mathbf{X} = 
    \begin{pmatrix}
        1 &X_{11} &\ldots &X_{1k} \\
        1 &X_{21} &\ldots &X_{2k} \\
        \vdots &\vdots &\vdots &\vdots \\
        1 &X_{n1} &\ldots &X_{nk}
    \end{pmatrix}\]
    In this notation, we have $k$ variables with total $n$ datas for each variable. Similarly we define,
    \[
        \mathbf{Y} =
        \begin{pmatrix}
            Y_1 \\
            Y_2 \\
            \vdots \\
            Y_n
        \end{pmatrix}
        \quad 
        \mathbf{\beta} = 
        \begin{pmatrix}
            \beta_0 \\
            \beta_1 \\
            \vdots \\
            \beta_k
        \end{pmatrix}
        \quad
        \mathbf{\epsilon} = 
        \begin{pmatrix}
            \epsilon_1 \\
            \epsilon_2 \\
            \vdots \\
            \epsilon_n
        \end{pmatrix}
    \]
    Then we can write our simple regression model with matrix notations, i.e
    \[\mathbf{Y} = \mathbf{ X\beta} + \mathbf{\epsilon} \]
    It is very convenient to work with the matrices. The next theorem, which is the generalization of the least square estimate expressions, shows that
    \begin{theorem}
        Assume that $\mathbf{X^TX}$ is invertible, then
        \begin{align*}
            \mathbf{\widehat{\beta}} \quad &  = \quad (\mathbf{X^TX})^{-1}\mathbf{X^TY} \\
            \V(\mathbf{\widehat{\beta}} | \mathbf{X^n}) \quad &  = \quad \sigma^2(\mathbf{X^TX})^{-1}
        \end{align*}
        \begin{proof}[Proof-Sketch]
            Let $S = \sum_{i=1}^n \widehat{\epsilon_i}^2$ \\
            Assuming the inverse of $\mathbf{X^TX}$ exists, we wish to minimize
            \[ S( \mathbf{\beta}) = \left( \mathbf{Y} - \mathbf{X\beta} \right)^2\]
            Opening the paranthesises, taking the derivative and equating to $0$, (note that $S$ is convex) proves the first part of the theorem. \\
            Second theorem uses the definiton variance and matrix manipulation (simply put).
        \end{proof}
    \end{theorem}
\section{References}
\begin{enumerate}
    \item \url{https://stats.stackexchange.com/questions/221891/difference-between-residual-and-disturbance-epsilon}
    \item \url{https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression}
    \item \url{https://igpphome.ucsd.edu/~cathy/Classes/SIO223A/sio223a.chap7.pdf}
    \item \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Unbiasedness_and_variance_of_%7F'%22%60UNIQ--postMath-00000037-QINU%60%22'%7F}
    \item \url{https://stats.stackexchange.com/questions/124818/logistic-regression-error-term-and-its-distribution}
    \item \url{https://stats.stackexchange.com/questions/23479/why-do-we-assume-that-the-error-is-normally-distributed}
    \item \url{https://stats.stackexchange.com/questions/148803/how-does-linear-regression-use-the-normal-distribution}
\end{enumerate}
